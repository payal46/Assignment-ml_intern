# -*- coding: utf-8 -*-
"""ML_intern_assignment .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZH_iP_EtcDJmTamicKB4qecZjduiEaJC
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV

# Load dataset
file_path = 'TASK-ML-INTERN.csv'
df = pd.read_csv(file_path)

# Drop non-numeric columns if present
if 'hsi_id' in df.columns:
    df.drop(columns=['hsi_id'], inplace=True)

# Handle missing values
df.fillna(df.mean(), inplace=True)

# Data distribution before outlier removal
plt.figure(figsize=(10, 5))
sns.boxplot(data=df)
plt.title('Feature Distribution Before Outlier Removal')
plt.xticks(rotation=90)
plt.show()

# Remove outliers using IQR method
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]

# Data distribution after outlier removal
plt.figure(figsize=(10, 5))
sns.boxplot(data=df)
plt.title('Feature Distribution After Outlier Removal')
plt.xticks(rotation=90)
plt.show()

# Separate features and target variable
X = df.drop(columns=['vomitoxin_ppb'])
y = df['vomitoxin_ppb']

# Normalize features
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# PCA for Dimensionality Reduction
pca = PCA(n_components=10)
X_pca = pca.fit_transform(X_scaled)

# Explained variance plot
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('PCA Explained Variance')
plt.show()

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

# Hyperparameter Tuning for Random Forest
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}
rf_model = RandomForestRegressor(random_state=42)
grid_search = RandomizedSearchCV(rf_model, param_grid, n_iter=10, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)
rf_best_model = grid_search.best_estimator_

# Feature Importance Plot
feature_importances = rf_best_model.feature_importances_
plt.figure(figsize=(8, 5))
sns.barplot(x=np.arange(1, 11), y=feature_importances)
plt.xlabel('Principal Components')
plt.ylabel('Importance')
plt.title('Feature Importance from Random Forest')
plt.show()

# Transformer Model
class TransformerRegressor(nn.Module):
    def __init__(self, input_dim):
        super(TransformerRegressor, self).__init__()
        self.embedding = nn.Linear(input_dim, 64)
        self.attention = nn.MultiheadAttention(embed_dim=64, num_heads=4)
        self.fc1 = nn.Linear(64, 32)
        self.fc2 = nn.Linear(32, 1)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        x = self.embedding(x)
        x = x.unsqueeze(0)
        x, _ = self.attention(x, x, x)
        x = x.squeeze(0)
        x = self.dropout(self.relu(self.fc1(x)))
        x = self.fc2(x)
        return x

# Convert data to tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

# Train Transformer Model
transformer_model = TransformerRegressor(X_train.shape[1])
criterion = nn.MSELoss()
optimizer = optim.Adam(transformer_model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)

for epoch in range(150):
    optimizer.zero_grad()
    outputs = transformer_model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()
    scheduler.step()

# Predictions
rf_pred = rf_best_model.predict(X_test)
transformer_pred = transformer_model(X_test_tensor).detach().numpy()

# Model Evaluation
rf_mae = mean_absolute_error(y_test, rf_pred)
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))
rf_r2 = r2_score(y_test, rf_pred)

transformer_mae = mean_absolute_error(y_test, transformer_pred)
transformer_rmse = np.sqrt(mean_squared_error(y_test, transformer_pred))
transformer_r2 = r2_score(y_test, transformer_pred)

# Scatter Plots for Predictions vs. Actual
plt.figure(figsize=(10, 5))
plt.scatter(y_test, rf_pred, alpha=0.6, label='Random Forest')
plt.scatter(y_test, transformer_pred, alpha=0.6, label='Transformer')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--', color='black')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Predicted vs. Actual Values')
plt.legend()
plt.show()

# Print Evaluation Metrics
print("Random Forest Performance:")
print(f'MAE: {rf_mae:.3f}, RMSE: {rf_rmse:.3f}, R²: {rf_r2:.3f}')

print("Transformer Performance:")
print(f'MAE: {transformer_mae:.3f}, RMSE: {transformer_rmse:.3f}, R²: {transformer_r2:.3f}')

